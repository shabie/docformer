{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/akarshu121/document-image-classification-with-docformer?scriptVersionId=97754630\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"<img src=\"https://images.unsplash.com/photo-1532153975070-2e9ab71f1b14?ixlib=rb-1.2.1&dl=annie-spratt-5cFwQ-WMcJU-unsplash.jpg&w=1920&q=80&fm=jpg&crop=entropy&cs=tinysrgb\">\n\n\n## 1. Introduction: \n* This notebook is a tutorial to the multi-modal architecture DocFormer (mainly for the purpose of Document Understanding).\n* We would take in, the test-images of the RVL-CDIP Dataset, and then would train the model on a subset of the dataset\n* We would also be logging the metrics with the help of Weights and Biases","metadata":{}},{"cell_type":"markdown","source":"## A small Introduction about the Model:\n\n<img src = \"https://github.com/uakarsh/docformer/raw/master/images/docformer-architecture.png\">\n\nDocFormer is a multi-modal transformer based architecture for the task of Visual Document Understanding (VDU). In addition, DocFormer is pre-trained in an unsupervised fashion using carefully designed tasks which encourage multi-modal interaction. DocFormer uses text, vision and spatial features and combines them using a novel multi-modal self-attention layer. DocFormer also shares learned spatial embeddings across modalities which makes it easy for the model to correlate text to visual tokens and vice versa. DocFormer is evaluated on 4 different datasets each with strong baselines. DocFormer achieves state-of-the-art results on all of them, sometimes beating models 4x its size (in no. of parameters).\n\nFor more understanding of the model and its code implementation, one can visit [here](https://github.com/uakarsh/docformer). So, let us go on to see what this model has to offer\n\nThe report for this entire run is attached [here](https://wandb.ai/iakarshu/RVL%20CDIP%20with%20DocFormer%20New%20Version/reports/Performance-of-DocFormer-with-RVL-CDIP-Test-Dataset--VmlldzoyMTI3NTM4)\n\n<img src = \"https://drive.google.com/u/1/uc?id=1IOyYXbU8bi5FDq59Z4RI1Qkoc54CzZto&export=download\" >\n\n### Installing the Libraries ‚öôÔ∏è:","metadata":{}},{"cell_type":"code","source":"## Installing the dependencies (might take some time)\n\n!pip install -q pytesseract\n!sudo apt install  -q tesseract-ocr\n!pip install  -q transformers\n!pip install  -q pytorch-lightning\n!pip install  -q einops\n!pip install  -q tqdm\n!pip install  -q 'Pillow==7.1.2'\n!pip install  -q datasets\n!pip install wandb\n!pip install torchmetrics","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Cloning the repository\n!git clone https://github.com/uakarsh/docformer.git","metadata":{"execution":{"iopub.status.busy":"2022-06-07T07:08:22.079626Z","iopub.execute_input":"2022-06-07T07:08:22.080057Z","iopub.status.idle":"2022-06-07T07:08:24.114693Z","shell.execute_reply.started":"2022-06-07T07:08:22.080014Z","shell.execute_reply":"2022-06-07T07:08:24.113469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Logging into wandb\n\nimport wandb\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"wandb_api\")\nwandb.login(key=secret_value_0)","metadata":{"execution":{"iopub.status.busy":"2022-06-07T07:08:24.145099Z","iopub.execute_input":"2022-06-07T07:08:24.146015Z","iopub.status.idle":"2022-06-07T07:08:25.65243Z","shell.execute_reply.started":"2022-06-07T07:08:24.145939Z","shell.execute_reply":"2022-06-07T07:08:25.651299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Libraries üìò:","metadata":{}},{"cell_type":"code","source":"## Importing the libraries\n\nimport warnings\nwarnings.simplefilter(\"ignore\", UserWarning)\nwarnings.simplefilter(\"ignore\", RuntimeWarning)\n\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nimport numpy as np\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset,DataLoader\n\nimport torch.nn.functional as F\nimport torchvision.models as models\n\n## Adding the path of docformer to system path\nimport sys\nsys.path.append('./docformer/src/docformer/')\n\n## Importing the functions from the DocFormer Repo\nfrom dataset import create_features\nfrom modeling import DocFormerEncoder,ResNetFeatureExtractor,DocFormerEmbeddings,LanguageFeatureExtractor\nfrom transformers import BertTokenizerFast","metadata":{"execution":{"iopub.status.busy":"2022-06-07T07:08:25.654629Z","iopub.execute_input":"2022-06-07T07:08:25.65569Z","iopub.status.idle":"2022-06-07T07:08:31.919208Z","shell.execute_reply.started":"2022-06-07T07:08:25.655654Z","shell.execute_reply":"2022-06-07T07:08:31.91827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Hyperparameters\n\nseed = 42\ntarget_size = (500, 384)\n\n## Setting some hyperparameters\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n## One can change this configuration and try out new combination\nconfig = {\n  \"coordinate_size\": 96,              ## (768/8), 8 for each of the 8 coordinates of x, y\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"image_feature_pool_shape\": [7, 7, 256],\n  \"intermediate_ff_size_factor\": 4,\n  \"max_2d_position_embeddings\": 1024,\n  \"max_position_embeddings\": 128,\n  \"max_relative_positions\": 8,\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"shape_size\": 96,\n  \"vocab_size\": 30522,\n  \"layer_norm_eps\": 1e-12,\n}","metadata":{"execution":{"iopub.status.busy":"2022-06-07T07:08:31.920835Z","iopub.execute_input":"2022-06-07T07:08:31.921618Z","iopub.status.idle":"2022-06-07T07:08:31.989757Z","shell.execute_reply.started":"2022-06-07T07:08:31.921575Z","shell.execute_reply":"2022-06-07T07:08:31.988868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## A small note üóíÔ∏è: \nHere, for the purpose of Demo I would be using only 250 Images per class, and would train the model on it. Definintely for a data hungry model such as transformers, such a small data is not enough, but let us see what are the results on it.","metadata":{}},{"cell_type":"code","source":"from tqdm.auto import tqdm\n\n## For the purpose of prediction\nid2label = []\nlabel2id = {}\n\ncurr_class = 0\n## Preparing the Dataset\nbase_directory = '../input/the-rvlcdip-dataset-test/test'\ndict_of_img_labels = {'img':[], 'label':[]}\n\nmax_sample_per_class = 250\n\nfor label in tqdm(os.listdir(base_directory)):\n    img_path = os.path.join(base_directory, label)\n    \n    count = 0\n    if label not in label2id:\n        label2id[label] = curr_class\n        curr_class+=1\n        id2label.append(label)\n        \n    for img in os.listdir(img_path):\n        if count>max_sample_per_class:\n            break\n            \n        curr_img_path = os.path.join(img_path, img)\n        dict_of_img_labels['img'].append(curr_img_path)\n        dict_of_img_labels['label'].append(label2id[label])\n        count+=1","metadata":{"execution":{"iopub.status.busy":"2022-06-07T18:38:57.319504Z","iopub.execute_input":"2022-06-07T18:38:57.319852Z","iopub.status.idle":"2022-06-07T18:39:02.430787Z","shell.execute_reply.started":"2022-06-07T18:38:57.319827Z","shell.execute_reply":"2022-06-07T18:39:02.429826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\ndf = pd.DataFrame(dict_of_img_labels)","metadata":{"execution":{"iopub.status.busy":"2022-06-07T07:08:36.612078Z","iopub.execute_input":"2022-06-07T07:08:36.612682Z","iopub.status.idle":"2022-06-07T07:08:36.621744Z","shell.execute_reply.started":"2022-06-07T07:08:36.612639Z","shell.execute_reply":"2022-06-07T07:08:36.620752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split as tts\ntrain_df, valid_df = tts(df, random_state = seed, stratify = df['label'], shuffle = True)","metadata":{"execution":{"iopub.status.busy":"2022-06-07T07:08:36.623323Z","iopub.execute_input":"2022-06-07T07:08:36.623888Z","iopub.status.idle":"2022-06-07T07:08:36.973838Z","shell.execute_reply.started":"2022-06-07T07:08:36.623845Z","shell.execute_reply":"2022-06-07T07:08:36.972854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = train_df.reset_index().drop(columns = ['index'], axis = 1)\nvalid_df = valid_df.reset_index().drop(columns = ['index'], axis = 1)","metadata":{"execution":{"iopub.status.busy":"2022-06-07T07:08:36.976098Z","iopub.execute_input":"2022-06-07T07:08:36.976661Z","iopub.status.idle":"2022-06-07T07:08:36.990587Z","shell.execute_reply.started":"2022-06-07T07:08:36.976608Z","shell.execute_reply":"2022-06-07T07:08:36.989443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Making the dataset üíΩ:\n\nThe main idea behind making the dataset is, to pre-process the input into a given format, and then provide the input to the model. So, simply just the image path, and the other configurations, and boom üí•, you would get the desired pre-processed input","metadata":{}},{"cell_type":"code","source":"## Creating the dataset\n\nclass RVLCDIPData(Dataset):\n    \n    def __init__(self, image_list, label_list, target_size, tokenizer, max_len = 512, transform = None):\n        \n        self.image_list = image_list\n        self.label_list = label_list\n        self.target_size = target_size\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.image_list)\n    \n    def __getitem__(self, idx):\n        img_path = self.image_list[idx]\n        label = self.label_list[idx]\n        \n        ## More on this, in the repo mentioned previously\n        final_encoding = create_features(\n            img_path,\n            self.tokenizer,\n            add_batch_dim=False,\n            target_size=self.target_size,\n            max_seq_length=self.max_len,\n            path_to_save=None,\n            save_to_disk=False,\n            apply_mask_for_mlm=False,\n            extras_for_debugging=False,\n            use_ocr = True\n    )\n        if self.transform is not None:\n            ## Note that, ToTensor is already applied on the image\n            final_encoding['resized_scaled_img'] = self.transform(final_encoding['resized_scaled_img'])\n        \n        \n        keys_to_reshape = ['x_features', 'y_features', 'resized_and_aligned_bounding_boxes']\n        for key in keys_to_reshape:\n            final_encoding[key] = final_encoding[key][:self.max_len]\n            \n        final_encoding['label'] = torch.as_tensor(label).long()\n        return final_encoding","metadata":{"execution":{"iopub.status.busy":"2022-06-07T07:08:36.992277Z","iopub.execute_input":"2022-06-07T07:08:36.993034Z","iopub.status.idle":"2022-06-07T07:08:37.006426Z","shell.execute_reply.started":"2022-06-07T07:08:36.992996Z","shell.execute_reply":"2022-06-07T07:08:37.005431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Defining the tokenizer\ntokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")","metadata":{"execution":{"iopub.status.busy":"2022-06-07T07:08:37.00823Z","iopub.execute_input":"2022-06-07T07:08:37.009227Z","iopub.status.idle":"2022-06-07T07:08:38.530076Z","shell.execute_reply.started":"2022-06-07T07:08:37.009184Z","shell.execute_reply":"2022-06-07T07:08:38.529039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchvision import transforms\n\n## Normalization to these mean and std (I have seen some tutorials used this, and also in image reconstruction, so used it)\ntransform = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n                              ","metadata":{"execution":{"iopub.status.busy":"2022-06-07T07:08:38.531653Z","iopub.execute_input":"2022-06-07T07:08:38.53226Z","iopub.status.idle":"2022-06-07T07:08:38.538266Z","shell.execute_reply.started":"2022-06-07T07:08:38.532219Z","shell.execute_reply":"2022-06-07T07:08:38.537005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds = RVLCDIPData(train_df['img'].tolist(), train_df['label'].tolist(),\n                      target_size, tokenizer, config['max_position_embeddings'], transform)\nval_ds = RVLCDIPData(valid_df['img'].tolist(), valid_df['label'].tolist(),\n                      target_size, tokenizer,config['max_position_embeddings'],  transform)","metadata":{"execution":{"iopub.status.busy":"2022-06-07T07:08:38.540023Z","iopub.execute_input":"2022-06-07T07:08:38.540791Z","iopub.status.idle":"2022-06-07T07:08:38.550323Z","shell.execute_reply.started":"2022-06-07T07:08:38.540746Z","shell.execute_reply":"2022-06-07T07:08:38.549236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Collate Function:\n\nDefinitely collate function is an amazing function for using the dataloader as per our wish. More on collate function can be known from [here](https://stackoverflow.com/questions/65279115/how-to-use-collate-fn-with-dataloaders)","metadata":{}},{"cell_type":"code","source":"def collate_fn(data_bunch):\n\n  '''\n  A function for the dataloader to return a batch dict of given keys\n\n  data_bunch: List of dictionary\n  '''\n\n  dict_data_bunch = {}\n\n  for i in data_bunch:\n    for (key, value) in i.items():\n      if key not in dict_data_bunch:\n        dict_data_bunch[key] = []\n      dict_data_bunch[key].append(value)\n\n  for key in list(dict_data_bunch.keys()):\n      dict_data_bunch[key] = torch.stack(dict_data_bunch[key], axis = 0)\n\n  return dict_data_bunch","metadata":{"execution":{"iopub.status.busy":"2022-06-07T07:08:38.552109Z","iopub.execute_input":"2022-06-07T07:08:38.552819Z","iopub.status.idle":"2022-06-07T07:08:38.566185Z","shell.execute_reply.started":"2022-06-07T07:08:38.552777Z","shell.execute_reply":"2022-06-07T07:08:38.561295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Defining the DataModule üìñ\n\n* A datamodule is a shareable, reusable class that encapsulates all the steps needed to process data:\n\n* A DataModule is simply a collection of a train_dataloader(s), val_dataloader(s), test_dataloader(s) and predict_dataloader(s) along with the matching transforms and data processing/downloads steps required.\n\n\n","metadata":{}},{"cell_type":"code","source":"import pytorch_lightning as pl\n\nclass DataModule(pl.LightningDataModule):\n\n  def __init__(self, train_dataset, val_dataset,  batch_size = 4):\n\n    super(DataModule, self).__init__()\n    self.train_dataset = train_dataset\n    self.val_dataset = val_dataset\n    self.batch_size = batch_size\n\n  def train_dataloader(self):\n    return DataLoader(self.train_dataset, batch_size = self.batch_size, \n                      collate_fn = collate_fn, shuffle = True)\n  \n  def val_dataloader(self):\n    return DataLoader(self.val_dataset, batch_size = self.batch_size,\n                                  collate_fn = collate_fn, shuffle = False)","metadata":{"execution":{"iopub.status.busy":"2022-06-07T07:08:38.567907Z","iopub.execute_input":"2022-06-07T07:08:38.568576Z","iopub.status.idle":"2022-06-07T07:08:39.574703Z","shell.execute_reply.started":"2022-06-07T07:08:38.568533Z","shell.execute_reply":"2022-06-07T07:08:39.573807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"datamodule = DataModule(train_ds, val_ds)","metadata":{"execution":{"iopub.status.busy":"2022-06-07T07:08:39.576481Z","iopub.execute_input":"2022-06-07T07:08:39.577345Z","iopub.status.idle":"2022-06-07T07:08:39.582506Z","shell.execute_reply.started":"2022-06-07T07:08:39.577298Z","shell.execute_reply":"2022-06-07T07:08:39.581255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. Modeling Part üèéÔ∏è\n\n1. Firstly, we would define the pytorch model with our configurations, in which the class labels would be ranging from 0 to 15\n2. Secondly, we would encode it in the PyTorch Lightening module, and boom üí• our work of defining the model is done","metadata":{}},{"cell_type":"code","source":"class DocFormerForClassification(nn.Module):\n  \n    def __init__(self, config):\n      super(DocFormerForClassification, self).__init__()\n\n      self.resnet = ResNetFeatureExtractor(hidden_dim = config['max_position_embeddings'])\n      self.embeddings = DocFormerEmbeddings(config)\n      self.lang_emb = LanguageFeatureExtractor()\n      self.config = config\n      self.dropout = nn.Dropout(config['hidden_dropout_prob'])\n      self.linear_layer = nn.Linear(in_features = config['hidden_size'], out_features = len(id2label))  ## Number of Classes\n      self.encoder = DocFormerEncoder(config)\n\n    def forward(self, batch_dict):\n\n      x_feat = batch_dict['x_features']\n      y_feat = batch_dict['y_features']\n\n      token = batch_dict['input_ids']\n      img = batch_dict['resized_scaled_img']\n\n      v_bar_s, t_bar_s = self.embeddings(x_feat,y_feat)\n      v_bar = self.resnet(img)\n      t_bar = self.lang_emb(token)\n      out = self.encoder(t_bar,v_bar,t_bar_s,v_bar_s)\n      out = self.linear_layer(out)\n      out = out[:, 0, :]\n      return out","metadata":{"execution":{"iopub.status.busy":"2022-06-07T07:08:39.58445Z","iopub.execute_input":"2022-06-07T07:08:39.584879Z","iopub.status.idle":"2022-06-07T07:08:39.597737Z","shell.execute_reply.started":"2022-06-07T07:08:39.584836Z","shell.execute_reply":"2022-06-07T07:08:39.596891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Defining pytorch lightning model\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport torchmetrics\n\nclass DocFormer(pl.LightningModule):\n\n  def __init__(self, config , lr = 5e-5):\n    super(DocFormer, self).__init__()\n    \n    self.save_hyperparameters()\n    self.config = config\n    self.docformer = DocFormerForClassification(config)\n    \n    self.num_classes = len(id2label)\n    self.train_accuracy_metric = torchmetrics.Accuracy()\n    self.val_accuracy_metric = torchmetrics.Accuracy()\n    self.f1_metric = torchmetrics.F1Score(num_classes=self.num_classes)\n    self.precision_macro_metric = torchmetrics.Precision(\n            average=\"macro\", num_classes=self.num_classes\n        )\n    self.recall_macro_metric = torchmetrics.Recall(\n            average=\"macro\", num_classes=self.num_classes\n        )\n    self.precision_micro_metric = torchmetrics.Precision(average=\"micro\")\n    self.recall_micro_metric = torchmetrics.Recall(average=\"micro\")\n\n  def forward(self, batch_dict):\n    logits = self.docformer(batch_dict)\n    return logits\n\n  def training_step(self, batch, batch_idx):\n    logits = self.forward(batch)\n\n    loss = nn.CrossEntropyLoss()(logits, batch['label'])\n    preds = torch.argmax(logits, 1)\n\n    ## Calculating the accuracy score\n    train_acc = self.train_accuracy_metric(preds, batch[\"label\"])\n\n    ## Logging\n    self.log('train/loss', loss,prog_bar = True, on_epoch=True, logger=True, on_step=True)\n    self.log('train/acc', train_acc, prog_bar = True, on_epoch=True, logger=True, on_step=True)\n\n    return loss\n  \n  def validation_step(self, batch, batch_idx):\n    logits = self.forward(batch)\n    loss = nn.CrossEntropyLoss()(logits, batch['label'])\n    preds = torch.argmax(logits, 1)\n    \n    labels = batch['label']\n    # Metrics\n    valid_acc = self.val_accuracy_metric(preds, labels)\n    precision_macro = self.precision_macro_metric(preds, labels)\n    recall_macro = self.recall_macro_metric(preds, labels)\n    precision_micro = self.precision_micro_metric(preds, labels)\n    recall_micro = self.recall_micro_metric(preds, labels)\n    f1 = self.f1_metric(preds, labels)\n\n    # Logging metrics\n    self.log(\"valid/loss\", loss, prog_bar=True, on_step=True, logger=True)\n    self.log(\"valid/acc\", valid_acc, prog_bar=True, on_epoch=True, logger=True, on_step=True)\n    self.log(\"valid/precision_macro\", precision_macro, prog_bar=True, on_epoch=True, logger=True, on_step=True)\n    self.log(\"valid/recall_macro\", recall_macro, prog_bar=True, on_epoch=True, logger=True, on_step=True)\n    self.log(\"valid/precision_micro\", precision_micro, prog_bar=True, on_epoch=True, logger=True, on_step=True)\n    self.log(\"valid/recall_micro\", recall_micro, prog_bar=True, on_epoch=True, logger=True, on_step=True)\n    self.log(\"valid/f1\", f1, prog_bar=True, on_epoch=True)\n    \n    return {\"label\": batch['label'], \"logits\": logits}\n\n  def validation_epoch_end(self, outputs):\n        labels = torch.cat([x[\"label\"] for x in outputs])\n        logits = torch.cat([x[\"logits\"] for x in outputs])\n        preds = torch.argmax(logits, 1)\n\n        wandb.log({\"cm\": wandb.sklearn.plot_confusion_matrix(labels.cpu().numpy(), preds.cpu().numpy())})\n        self.logger.experiment.log(\n            {\"roc\": wandb.plot.roc_curve(labels.cpu().numpy(), logits.cpu().numpy())}\n        )\n        \n  def configure_optimizers(self):\n    return torch.optim.AdamW(self.parameters(), lr = self.hparams['lr'])","metadata":{"execution":{"iopub.status.busy":"2022-06-07T07:09:12.786194Z","iopub.execute_input":"2022-06-07T07:09:12.786554Z","iopub.status.idle":"2022-06-07T07:09:12.869343Z","shell.execute_reply.started":"2022-06-07T07:09:12.786523Z","shell.execute_reply":"2022-06-07T07:09:12.86839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6. Summing it up and running the entire procedure üèÉ","metadata":{}},{"cell_type":"code","source":"from pytorch_lightning.callbacks import ModelCheckpoint\nfrom pytorch_lightning.callbacks.early_stopping import EarlyStopping\nfrom pytorch_lightning.loggers import WandbLogger\n\ndef main():\n    datamodule = DataModule(train_ds, val_ds)\n    docformer = DocFormer(config)\n\n    checkpoint_callback = ModelCheckpoint(\n        dirpath=\"./models\", monitor=\"valid/loss\", mode=\"min\"\n    )\n    early_stopping_callback = EarlyStopping(\n        monitor=\"valid/loss\", patience=3, verbose=True, mode=\"min\"\n    )\n    \n    wandb.init(config=config, project=\"RVL CDIP with DocFormer New Version\")\n    wandb_logger = WandbLogger(project=\"RVL CDIP with DocFormer New Version\", entity=\"iakarshu\")\n    ## https://www.tutorialexample.com/implement-reproducibility-in-pytorch-lightning-pytorch-lightning-tutorial/\n    pl.seed_everything(seed, workers=True)\n    trainer = pl.Trainer(\n        default_root_dir=\"logs\",\n        gpus=(1 if torch.cuda.is_available() else 0),\n        max_epochs=1,\n        fast_dev_run=False,\n        logger=wandb_logger,\n        callbacks=[checkpoint_callback, early_stopping_callback],\n        deterministic=True\n    )\n    trainer.fit(docformer, datamodule)","metadata":{"execution":{"iopub.status.busy":"2022-06-07T07:09:21.086225Z","iopub.execute_input":"2022-06-07T07:09:21.086667Z","iopub.status.idle":"2022-06-07T07:09:21.097492Z","shell.execute_reply.started":"2022-06-07T07:09:21.086636Z","shell.execute_reply":"2022-06-07T07:09:21.096609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    main()","metadata":{"execution":{"iopub.status.busy":"2022-06-07T07:09:21.297147Z","iopub.execute_input":"2022-06-07T07:09:21.299276Z","iopub.status.idle":"2022-06-07T07:13:58.468599Z","shell.execute_reply.started":"2022-06-07T07:09:21.299227Z","shell.execute_reply":"2022-06-07T07:13:58.467592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## References:\n\n1. [MLOps Repo](https://github.com/graviraja/MLOps-Basics) (For the integration of model and data with PyTorch Lightening) \n2. [PyTorch Lightening Docs](https://pytorch-lightning.readthedocs.io/en/stable/index.html) For all the doubts and bugs\n3. [My Repo](https://github.com/uakarsh/docformer) For downloading the model and pre-processing steps\n4. Unspash for Images\n5. Google for other stuffs","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}